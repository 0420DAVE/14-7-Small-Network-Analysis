---
title: "Language and Empathy: Network Analysis"
author: "Li Ying"
date: "12 July 2015"
output: html_document
---
```{r load data,echo=FALSE,message=FALSE}
library(xlsx); library(hash); library(dplyr); library(ggplot2); library(corrplot);library(vegan);library(igraph);library(BSDA)
setwd("~/Dropbox/#0")
source("global.R")

# Load data
word<-read.xlsx("./data.xlsx",sheetIndex=4,header=FALSE,colIndex=2:1000,rowIndex=4:76)
#word<-read.xlsx("./data.xlsx",sheetIndex=4,header=FALSE,colIndex=2:1000,rowIndex=4:76)
cues<-1:73
word<-data.frame(cue=cues,word)


# Process Empathy data 
empathy<-read.xlsx("./data.xlsx", sheetIndex=3, header=T, colIndex=2:31, rowIndex=3:36)
colnames(empathy)<-1:30
# remove participants with invalid input
empathy<-empathy[-c(6,21),]
# convert all column to numeirc
empathy[,20]<-as.numeric(empathy[,20]);empathy[,17]<-as.numeric(empathy[,17])
# Invert score of question 4, 9, 13, 16, 20, 27
empathy[,c(4,9,13,16,20,27)]<-6-empathy[,c(4,9,13,16,20,27)]
# Calculate mean following scoring instruction
empathy<-mutate(empathy,
                agg=round(apply( empathy,1,mean),digit=2),
                suffer=round(apply( empathy[,c(3,5,6,8,12,18,24,28)],1,mean),digit=2),
                positive.sharing=round(apply( empathy[,c(14, 22, 23, 29, 30)],1,mean),digit=2),
                responsive.crying=round(apply( empathy[,c(1,20,25)],1,mean),digit=2),
                emotional.attention=round(apply( empathy[,c(4,9,13,27)],1,mean),digit=2),
                fell.for.others=round(apply( empathy[,c(10,16,15,21)],1,mean),digit=2),
                emotional.contagion=round(apply( empathy[,c(11,17)],1,mean),digit=2)
                )

empathy<-empathy[,31:37]
```

### Executive Summary 
This report performed exploratory analysis on language pattern of two groups of people: high in empathy and low in empathy. Minimum spanning tree, hierachical clustering graph, and association network were plotted. In the end, statistical text were performed on parameters of association network using bootstrape with 1000 iteration. 

The statistical text shows that among three parameters of association network graph (clustering coefficient, diameter, and average path length), only APL shows significant difference between two groups (see table below)

```{r,echo=FALSE}
table
```

### Data Source
- Participant number: 33
- `Multi-Dimensional Emotional Scale of Empathy`: 30 questions with 6 questions to be reversed to reduce bias. [Scoreing instruction](http://www.unh.edu/emotional_intelligence/EI%20Assets/Emapthy%20Scale/Empathy%20Article%202000.doc)
- `Free Association task`: each participants were given 73 **cue words**. For each of cue words, they were instructed to write down 3 words/**responses** that they think is most relevant to the **cue words**. The result will be mentioned as **word data** in the following text.


### Data Process
- Empathy scale were averaged for each participants. Participants were categorized into High empathy group and Low empathy group using group mean as cut-off line. 
- The rest of process is identical to `free will and language: network analysis`

###### The distribution of empathy scale follows normal distribution

```{r test normality,echo=FALSE,message=FALSE,fig.height=4, fig.width=8, warning=FALSE, error=FALSE}

par(mfrow=c(1,2))
hist(empathy$agg,breaks=6,main="Average Empathy Score",xlab="empathy",col="steelblue",border =1); qqnorm(empathy$agg,col="steelblue");qqline(empathy$agg,col="red")
```

```{r prepare data,echo=FALSE,message=FALSE}

for (i in 1:31){
  if (empathy$agg[i] > mean(empathy$agg)){
    empathy$ctg[i]<-"High"
  }else{
    empathy$ctg[i]<-"Low"
  }
}
#table(empathy$ctg)
Hindex<-which (empathy$ctg %in% "High")
Lindex<-which (empathy$ctg %in% "Low")

# Transform index to fit `word` dataset 
f<-function(x){
  c(3*(x-1)+1, 3*(x-1)+2,3*x)}

Hindex<-f(Hindex)
Lindex<-f(Lindex)

# Remove participants 6 and 21 from word dataset
word<-word[,-c(f(c(6,21)) )]

# split word data by high/low empathy score
Hword<-word[,c(1,Hindex+1)]
Lword<-word[,c(1,Lindex+1)]

#Hindex<-sample(1:93,47) #
#Lindex<-c(1:93)[-Hindex] #
#Hword<-word[,c(1,Hindex+1)] #
#Lword<-word[,c(1,Lindex+1)] #

Hfreq<-process(Hword)
Lfreq<-process(Lword)
freq<-process(word)

```


```

### Exploratory Data Analysis

#### Clustering Analysis (single linkage)
Cluster Dendrogram were plotted. Both groups shows that most clue words merges at a high y-axis, suggesting similarity between clue words are quite low. 
```{r  Clustering Analysis,echo=FALSE,message=FALSE}
#dev.off()
par(mar=c(2,2,2,2))
par(mfrow=c(2,1))
dis <- vegdist(t(Hfreq)); clus <- hclust(dis, "average"); plot(clus,cex=0.6,main="High Free Will Group"); 
rect.hclust(clus,10) # Inspect at five levels 


dis <- vegdist(t(Lfreq)); clus <- hclust(dis, "average"); plot(clus,cex=0.6,main="Low Free Will Group"); 
rect.hclust(clus,10) # Inspect at five levels 
```

### Minimum Spanning Trees (MST)
Two groups looks similar in MST
```{r draw MST, echo=FALSE,message=FALSE,warning=FALSE,error=FALSE}
#dev.off()
par(mar=c(2,2,2,2))
par(mfrow=c(1,1))
MST(Hfreq, "High") 
MST(Lfreq, "Low")
```

#### Association Network Graph
- Association Network were only plotted on those cue words that have correlation > 0.2
- The edge were colored according to correlatonal strenth between nodes, followed sequence of red, blue and grey representing correlation >0.5, >0.4, and else.
- The length of edge is weighted by correlational strength between nodes. 

Method: [Tutorial](http://stackoverflow.com/questions/19965600/igraph-r-how-to-create-correlation-network-with-only-strong-r-values)

```{r plot network,echo=FALSE,message=FALSE}
par(mfrow=c(1,1))
par(mar=c(1,1,1,1))
AssocNetPlot(Hfreq,"High")
AssocNetPlot(Lfreq,"Low")
#mtext("My 'Title' in a strange place", side = 3, line = 5, outer = TRUE)
```
      
      
##### Network parameters from HIGH and LOW groups were calculated:
```{r calculateing parameter,echo=FALSE,message=FALSE}
Hg<-graph(Hfreq)
Lg<-graph(Lfreq)
g<-graph(freq)

hAPL<-average.path.length(Hg); lAPL<-average.path.length(Lg)
hD<-diameter(Hg); lD<-diameter(Lg)
hCC<-transitivity(Hg); lCC<-transitivity(Lg)
hdegree<-mean(degree(Hg)) ;ldegree<- mean(degree(Lg))

I<-c(hAPL,lAPL,hD,lD,hCC,lCC,hdegree,ldegree)
compare<-matrix(I,nrow=4,byrow=T)
colnames(compare)<-c("High","Low")
rownames(compare)<-c("APL","Diameter","CC","Degree")
compare
```

##### Degree distribution
Most of node do not have a lot edges with other node. But there were some nodes have lots of edges with others. Those nodes were hubs in the network that connects between different clusters.  
```{r}
par(mar=c(2,2,2,2))
par(mfrow=c(2,2))
hist(degree(Hg),col="steelblue", main="Degree Histogram | High", xlab="degree/number of edge")
plot(degree.distribution(Hg),main="high",col="steelblue")

hist(degree(Lg),col="steelblue", main="Degree Histogram | Low", xlab="degree/number of edge")
plot(degree.distribution(Lg), main="low",col="steelblue")


```

#### Statistical text on social network graph
All parameters shows statistical significance between two groups (p-value < 0.0001)

```{r bootstrap statistical test,echo=FALSE,message=FALSE}
NodeIndex<-1:dim(freq)[2]

comAPL<-matrix(rep(0,2000,),ncol=2);colnames(comAPL)<-c("High","Low")
comD<-matrix(rep(0,2000,),ncol=2);colnames(comD)<-c("High","Low")
comCC<-matrix(rep(0,2000,),ncol=2);colnames(comCC)<-c("High","Low")

set.seed(134)
for (i in 1:1000){
  ind<-sample(NodeIndex,32,replace=F)
  BHfreq<-Hfreq[,ind]
  BLfreq<-Lfreq[,ind]
  BHg<-graph(BHfreq)
  BLg<-graph(BLfreq)
  comAPL[i,1]=average.path.length(BHg)
  comAPL[i,2]=average.path.length(BLg)
  
  comD[i,1]=diameter(BHg)
  comD[i,2]=diameter(BLg)
  
  comCC[i,1]=transitivity(BHg)
  comCC[i,2]=transitivity(BLg)
  
  }

# Normality test
#hist(comAPL[,1],breaks=100)
#hist(comAPL[,2],breaks=100)
#qqnorm(comAPL[,1], col="blue")
#qqnorm(comAPL[,2], col="blue")


# t-test on average path length
#z.test(comAPL[,1],comAPL[,2],sigma.x=sd(comAPL[, 1]),sigma.y=sd(comAPL[, 1]))
APL<-t.test(comAPL[,1],comAPL[,2])
# t-test on diameter 
D<-t.test(comD[,1],comD[,2])
# t-test on cluster coefficient 
CC<-t.test(comCC[,1],comCC[,2])

```

##### Summary:
```{r stat summary,echo=FALSE,message=FALSE}
table<-data.frame(High.Mean=c( APL$estimate[[1]], D$estimate[[1]], CC$estimate[[1]] ),
              Low.Mean=c( APL$estimate[[2]], D$estimate[[2]], CC$estimate[[2]] ),
              p.value=c(format(APL$p.value, scientific=F,digits=1),
                        format(D$p.value, scientific=F,digits=5), 
                        format(CC$p.value, scientific=F, digits=6)))
table<-as.matrix(table)
rownames(table)<-c("APL","D","CC")
table
```

#### Detailed t-test result
```{r,echo=FALSE,message=FALSE,error=FALSE,warning=FALSE}
# t-test on average path length
t.test(comAPL[,1],comAPL[,2])
# t-test on diameter 
t.test(comD[,1],comD[,2])
# t-test on cluster coefficient 
t.test(comCC[,1],comCC[,2])
```

   
      
### Appendix
- [Scoreing instruction](http://www.unh.edu/emotional_intelligence/EI%20Assets/Emapthy%20Scale/Empathy%20Article%202000.doc)
1. I cry easily when watching a sad movie.  	 
2. Certain pieces of music can really move me.	 
3. Seeing a hurt animal by the side of the road is very upsetting. 	 
4. R. I don't give others' feelings much thought.  		 
5. It makes me happy when I see people being nice to each other.	 
6. The suffering of others deeply disturbs me.
7. I always try to tune in to the feelings of those around me.	 
8. I get very upset when I see a young child who is being treated meanly.	
9. R. Too much is made of the suffering of pets or animals.	 
10. If someone is upset I get upset, too.	
11. When I'm with other people who are laughing I join in.	 
12. It makes me mad to see someone treated unjustly.	 
13. R. I rarely take notice when people treat each other warmly. 
14. I feel happy when I see people laughing and enjoying themselves.	 
15. It's easy for me to get carried away by other people's emotions. 
16. R. My feelings are my own and don’t reflect how others feel.  
17. If a crowd gets excited about something so do I.			 
18. I feel good when I help someone out or do something nice for someone.  		 
19. I feel deeply for others.	 
20. R. I don't cry easily.  	 
21. I feel other people's pain. 
22. Seeing other people smile makes me smile.	
23. Being around happy people makes me feel happy, too.	 
24. TV or news stories about injured or sick children greatly upset me.				 
25. I cry at sad parts of the books I read.		 
26. Being around people who are depressed brings my mood down.	 
27. R. I find it annoying when people cry in public.	 
28. It hurts to see another person in pain.	 
29. I get a warm feeling for someone if I see them helping another person.  	 
30. I feel other people's joy.	 

Note: R indicates a reverse-scored item. To score the scale, change the scoring on the reverse-scored items (1=5, 2=4, 3=3, 4=2, 5=1). Add all the scores for the Total score and divide by 30.  Add the following items together for each scale, and divide by the number of items: Suffering ( 3, 5, 6, 8, 12, 18, 24, 28); Positive Sharing (14, 22, 23, 29, 30); Responsive Crying (1, 20, 25); Emotional Attention (4, 9, 13, 27); Feel for Others (10, 15, 16, 21); Emotional Contagion (11, 17).  Take the mean of these sub-scales to compute a General Empathy scale. 